{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we set to explore the topography of reddit. We wanted to see what subreddits are commonly used between users, if there are communities of users that act as links between subreddits, and if there are subreddits isolated from the main users' network of Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in clustering subreddits, by using comment and submission data to discern connections between subreddits and the users who are active in them. We are hoping to see similar userbases in subreddits we did not expect, along with disconnects in userbases between subreddits that appear intuitively similar. We want to use a Clustering algorithm to connect subreddits in space, and then calculate the distance between subreddits.We are interested in clustering subreddits, by using comment and submission data obtained from PRAW to discern connections between subreddits and the users who are active in them. We are hoping to see similar userbases in subreddits we did not expect, along with disconnections in userbases between subreddits that appear intuitively similar. We want to use a Clustering algorithm to connect subreddits in space, and then calculate the distance between subreddits. \n",
    "\n",
    "For the former part of research, we mainly focused on obtaining data set including reddit user, their comments and submissions for subreddits, cleaning out unnecessary features and pre-processing data for further exploration. Simple visualization of reddit posts distribution, relations between user posts number and posted subreddit number was displayed, which none of them has suggested significant results so far. For over 22,000 subreddits, there is a large portion with few data included, and handling this majority is believed to affect the following data analysis significantly. Multi-dimensional vectors implemented with array structure are also the major sources to perform data exploration and clustering on.\n",
    "\n",
    "For the latter part, the main purpose will be completing data pre-processing,  figuring out correct application of Clustering Algorithm(method to choose, cluster number etc.) and displaying final visualization. The statistical analysis should also suggest some insights into the correlation among different subreddits and their active user community.\n",
    "\n",
    "Several questions are expected to be answered while we proceed:\n",
    "\n",
    "- How should we filter out the large amount of subreddits with few comments by simply removing the data or considering imputation methods? How should we determine a proper threshold for unpopular subreddits?\n",
    "\n",
    "- K-means Clustering Algorithm seems to work for current data, but instead of presuming the number of classification arbitrarily, is there any statistical domain knowledge or library function we can refer to for more reliable output?\n",
    "\n",
    "- The visualization in 2D space apparently fails to display more original properties for our multidimensional vectors. We need to implement proper functions to display more comprehensive data visualization, perhaps by using dimensionality reduction approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our data is coming from reddit. Reddit submission and comment data is publicly accessible, and reddit has a nice API structure. We are using the package PRAW (python reddit api wrapper), which makes the reddit api calls easier to use and python importable. We are attempting to build subreddit comment vectors for a large number of reddit users. We would like to create nested dictionaries, the first level key being a reddit user, the second level keys being a subreddit name, and the values being how many time that specific user posted to a specific subreddit. \n",
    "\n",
    "Another important note is there are two kinds of ways to post to reddit, and the api distinguishes them. There are submissions and comments. Submissions are posts including an image, a video, or a question, and comments are replies and follow-ups to posts. Both kinds of posts on reddit are useful for the clustering we want to do, so we must do both.\n",
    "\n",
    "We split our data scripts into different pieces, below is the documentation for each script, followed by the code. \n",
    "\n",
    "Also to gather any data from Reddit we must log on to the system with a developer id and create a reddit instance in our code. This happens once below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "reddit = praw.Reddit(client_id='tc_fFbWZrkDSRw',\n",
    "                     client_secret='fTq7nFVzdkCHFZY7jWQvHmkLpwk',\n",
    "                     user_agent='lhimelman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# userNameScraper.py:\n",
    "  *A Scraper that gets just usernames, does it very quickly. 6000 usernames can be collected in a few minutes*\n",
    "\n",
    "  EX: \n",
    "      \n",
    "      \n",
    "      python3 userNameScraper.py *saveFilename* *ListofSubreddit* *numberofpoststolookat*\n",
    "      \n",
    "      python3 userNameScraper.py data.txt funny,pics,todayilearned 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 post\n",
      "1 subreddit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Thoramel',\n",
       " 'Hrekires',\n",
       " 'Sip_py',\n",
       " 'dallasmorningnews',\n",
       " 'jimbozak',\n",
       " 'TransQuantinentalAce',\n",
       " 'darkseadrake',\n",
       " 'GotOutOfCowtown',\n",
       " 'becauseineedone3',\n",
       " 'bivalve_attack',\n",
       " 'TrumpImpeachedAugust',\n",
       " 'DistillateMedia',\n",
       " 'wjbc',\n",
       " 'neverliveindoubt',\n",
       " 'HorsecockBillionaire',\n",
       " 'MaimedJester',\n",
       " 'PrincessSandySparkle',\n",
       " 'Burning_Lovers',\n",
       " 'Choco316',\n",
       " 'Communist99',\n",
       " 'esteban1386',\n",
       " 'Schkateboarda',\n",
       " 'nuncio-tc',\n",
       " 'IMAVINCEMCMAHONGUY',\n",
       " 'all2neat',\n",
       " 'not-working-at-work',\n",
       " 'garybusey42069',\n",
       " 'erratically_sporadic',\n",
       " '10iss',\n",
       " 'Vernacularry',\n",
       " 'wisdom_and_frivolity',\n",
       " 'halebara01',\n",
       " 'ThrowAway_Phone',\n",
       " 'ericolinn',\n",
       " 'KellyJoyCuntBunny',\n",
       " 'JacenGraff',\n",
       " 'Roidciraptor',\n",
       " 'TempAcct20005']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrapeUsers(reddit, subredditList, postNum):\n",
    "    subnum = 0\n",
    "    for subredditname in subredditList:\n",
    "        users = []\n",
    "        posts = reddit.subreddit(subredditname).hot(limit=postNum)\n",
    "        pc = 0\n",
    "        for submission in posts:\n",
    "            all_comments = submission.comments.list()\n",
    "            for c in all_comments:\n",
    "                try:\n",
    "                    name = c.author.name\n",
    "                    if name not in users:\n",
    "                        users.append(name)\n",
    "                except:\n",
    "                    pass\n",
    "            pc = pc + 1\n",
    "            print( pc, \"post\")\n",
    "        subnum = subnum + 1\n",
    "        print( subnum, \"subreddit\")\n",
    "    return users\n",
    "\n",
    "##An example call scraping one post from r/funny\n",
    "users = scrapeUsers(reddit,['politics'],1)\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScrapeFreqfromUser.py:\n",
    "  *A scraper that gets frequencies of comments from a list of users*\n",
    "\n",
    "  EX: \n",
    "    \n",
    "        python3 ScrapeFreqfromUser.py *savefielName* *userlistfilename*\n",
    "     \n",
    "    \n",
    "        python3 ScrapeFreqfromUser.py freq.txt users.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 out of 3\n",
      "3 out of 3\n"
     ]
    }
   ],
   "source": [
    "def scrapeSubreddit(reddit, users):\n",
    "    commentFreq = {}\n",
    "    headers = []\n",
    "    usernum = 1\n",
    "    for user in users:\n",
    "        userCFreq = {}\n",
    "        for comment in reddit.redditor(user).comments.new(limit=None):\n",
    "            sub = comment.subreddit\n",
    "            if sub not in userCFreq:\n",
    "                userCFreq[sub] = 1\n",
    "            else:\n",
    "                userCFreq[sub] += 1\n",
    "            if sub not in headers:\n",
    "                headers.append(sub)\n",
    "        commentFreq[user] = userCFreq\n",
    "        usernum = usernum + 1\n",
    "        print(usernum, \"out of\", len(users))\n",
    "    return commentFreq,headers\n",
    "\n",
    "##An example call scraping the users gotten above\n",
    "cfreq,headers = scrapeSubreddit(reddit, users[0:3])\n",
    "df = pd.DataFrame.from_dict(data=cfreq, orient='index').fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScrapeSubFreqfromUser.py:\n",
    "  *A scraper that gets frequencies of submissions from a list of users*\n",
    "\n",
    "  EX: \n",
    "    \n",
    "        python3 ScrapeSubFreqfromUser.py *savefielName* *userlistfilename*\n",
    "     \n",
    "    \n",
    "        python3 ScrapeSubFreqfromUser.py freq.txt users.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeSubreddit(reddit, users):\n",
    "    subFreq = {}\n",
    "    headers = []\n",
    "    usernum = 1\n",
    "    for user in users:\n",
    "        userCFreq = {}\n",
    "        for submission in reddit.redditor(user).submissions.new(limit=None):\n",
    "            sub = submission.subreddit\n",
    "            if sub not in userCFreq:\n",
    "                userCFreq[sub] = 1\n",
    "            else:\n",
    "                userCFreq[sub] += 1\n",
    "            if sub not in headers:\n",
    "                headers.append(sub)\n",
    "        subFreq[user] = userCFreq\n",
    "        usernum = usernum + 1\n",
    "        print(usernum, \"out of\", len(users))\n",
    "    return subFreq,headers\n",
    "\n",
    "subfreq,headers = scrapeSubreddit(reddit, users[0:3])\n",
    "df = pd.DataFrame.from_dict(data=subfreq, orient='index').fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have two sparse matrices in which each column is a vector for an individual subreddit that contains frequencies of different users posting to or commenting on that subreddit.\n",
    "\n",
    "***\n",
    "The above calls are examples of running our scripts, but of course our actual data sets, (which we only want to pull down once), are much larger.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data came to us pretty clean. Reddit's api allows us to filter deleted comments and such. Our data cleaning and preprocessing included three different tasks. Below is the first 100 rows of our large table, and then a description of each task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf = pd.read_csv('../data.csv',nrows=100)\n",
    "bigdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task One:\n",
    "    \n",
    "    Create a method for removing very sparse vectors from our dataset. In looking through the data, we realized that there are some subreddits with really very few posts, that appears in our set of vectors without really doing anything. We decided to test clustering with the whole set and with smaller sets, so we made a way of thresholding how many posts a subreddit needs to be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold = 10\n",
    "\n",
    "def delSparse(df, threshold):\n",
    "    for c in list(df)[1:]:\n",
    "        if sum(list(df[c])) < Threshold:\n",
    "            del df[c]\n",
    "            \n",
    "delSparse(bigdf,Threshold)\n",
    "bigdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Two:\n",
    "    Format data so it is in the form expected by the clustering algorithm. The following code takes the dataframe and changes it to a numpy array. The code also saves a list of headers for referencing specific nodes in a cluster later.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "def changetoVec(df):\n",
    "    vectors = []\n",
    "    for c in list(df)[1:]:\n",
    "        vectors.append(list(df[c]))\n",
    "\n",
    "    return array(vectors)\n",
    "\n",
    "bigdfVec = changetoVec(bigdf)\n",
    "print(bigdfVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Three:\n",
    "    Remove Porn. What we discovered is that most porn subreddits fell under the category of incredibly sparse vectors, and so were removed, the ones that we decided not to include in our analysis for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can try clustering:\n",
    "    As it happens running unsupervised learning is not that complicated. We can run the clustering algorithm on our vectors fairly easy, but there are two problems. The first is we dont know what the optimal number of clusters our algorithm should produce is. We must find a way to choose a cluster number with the least error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "##A function that clusters with a given K\n",
    "def Cluster(vectors, Num_clusters):\n",
    "    whitened = whiten(vectors)\n",
    "    codebook, distortion = kmeans(whitened, Num_clusters)\n",
    "    return codebook, distortion\n",
    "\n",
    "##An example of the centroids returned by clustering\n",
    "codebook, dist = Cluster(bigdfVec[1:1000],5)\n",
    "whitened = whiten(bigdfVec[1:100])\n",
    "\n",
    "## The list of centroids\n",
    "print(codebook)\n",
    "\n",
    "#The estimated error of the clustering given that k value\n",
    "print(dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Distortion value returned from the k-means algorithm is the value that represents the error of that many clusters. So our first thought is to simply minimize that error. Below we create a list of all possible distortions for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####WARNING, THIS TAKES A WHILE\n",
    "\n",
    "##Get all distortions\n",
    "distortions = []\n",
    "for i in range(1,int(len(list(bigdfVec))/2)):\n",
    "    cb, dist = Cluster(bigdfVec, i)\n",
    "    distortions.append(dist)\n",
    "plt.plot(distortions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see the problem with just choosing the minimum here. The minimum will always be the same number of clusters as datapoints! This number of clusters isn't useful however, as having the same number of clusters as datapoints, is 100% overfitting. A common technique to combat this used alongside k-means, is finding when the maximum jump appears of inverted distortiong values. Below is the code to find the number of clusters when that point occurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for the maximum distortion jump. This should be our best k.\n",
    "\n",
    "maxdistjump = 0\n",
    "maxk = 0\n",
    "for i in range(1, len(distortions)):\n",
    "    if distortions[i] - distortions[i-1] > maxdistjump:\n",
    "        maxdistjump = distortions[i] - distortions[i-1]\n",
    "        maxk = i\n",
    "        \n",
    "print(maxk)\n",
    "print(maxdistjump)\n",
    "\n",
    "finalcb, finaldist = Cluster(bigdfVec, maxk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our theoretically best K value. Lets compute distances between each subreddit and cluster centers, then look at which subreddits are most in a specific cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##compute distances between every subreddit and cluster centers for the best k we found\n",
    "\n",
    "Clusterdistframe = pd.DataFrame(columns=list(bigdf)[1:])\n",
    "for i in range(len(finalcb)):\n",
    "    dists = []\n",
    "    for v2 in bigdfVec:\n",
    "        dists.append(distance.euclidean(finalcb[i], v2))\n",
    "    Clusterdistframe.loc[i] = dists\n",
    "Clusterdistframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the ten minimum and the ten maximum subreddits for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mins = pd.DataFrame()\n",
    "maxs = pd.DataFrame()\n",
    "for index,row in Clusterdistframe.iterrows():\n",
    "    mins[index] = list(Clusterdistframe.columns[row.argsort()][0:100])\n",
    "    maxs[index] = list(Clusterdistframe.columns[row.argsort()][-99:])\n",
    "    \n",
    "mins=mins.transpose()\n",
    "maxs=maxs.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del maxs[2]\n",
    "del maxs[3]\n",
    "del maxs[4]\n",
    "del maxs[5]\n",
    "maxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So theres a problem. By looking at this list we can see that there are unique clusters, but many of the clusters contain the same minimum subreddits, or they contain the same maximum subreddits. For example, there are many centroids very close to mildy_infuriating, which in and of itself is mildly infuriating. More importantly however, outliers appear to be super obvious. In that there are no cluster centers anywhere close to some subreddits, like Unity3D, and Denmark. Despite the fact that 297 clusters is mathematically the right number of clusters. It is harder to interpret on a human scale, so we also decided to guess a small number of clusters and look at the subreddits that appeared as minimums and maximums with less clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##cluster\n",
    "guesscb, guessdist = Cluster(bigdfVec, 10)\n",
    "\n",
    "##Get distances\n",
    "guessClusterdistframe = pd.DataFrame(columns=list(bigdf)[1:])\n",
    "for i in range(len(guesscb)):\n",
    "    dists = []\n",
    "    for v2 in bigdfVec:\n",
    "        dists.append(distance.euclidean(guesscb[i], v2))\n",
    "    guessClusterdistframe.loc[i] = dists\n",
    "\n",
    "##Get mins and maxs\n",
    "gmins = pd.DataFrame()\n",
    "gmaxs = pd.DataFrame()\n",
    "for index,row in guessClusterdistframe.iterrows():\n",
    "    gmins[index] = list(guessClusterdistframe.columns[row.argsort()][0:100])\n",
    "    gmaxs[index] = list(guessClusterdistframe.columns[row.argsort()][-99:])\n",
    "    \n",
    "gmins=gmins.transpose()\n",
    "gmaxs=gmaxs.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del gmaxs[2]\n",
    "del gmaxs[3]\n",
    "del gmaxs[5]\n",
    "gmaxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are a little more interesting to look at, in that there is a significant difference between the minimum subreddits that appear in each cluster. The maximums still arent that interesting, as those outlier subreddits are still really far away. This distance could indicate that those subreddits have pretty isolated userbases, which honestly makes sense. Politics is a subreddit that appears at a very high distance from most other subreddits. This implies that lots of users post on politics, and only politics. Which is certainly very possible, and in fact very likely.\n",
    "\n",
    "The next thing we thought would be interesting is to not only plot subreddits to cluster centers, but subreddits to eachother, to see total minimum distances and maximum distances between subreddits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "##compute distances between every subreddit and put them in a massive table\n",
    "\n",
    "distframe = pd.DataFrame(columns=list(bigdf)[1:])\n",
    "for i in range(len(bigdfVec)):\n",
    "    dists = []\n",
    "    for v2 in bigdfVec:\n",
    "        dists.append(distance.euclidean(bigdfVec[i], v2))\n",
    "    distframe.loc[i] = dists\n",
    "distframe.index = list(bigdf)[1:]\n",
    "distframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_mins = pd.DataFrame()\n",
    "sub_maxs = pd.DataFrame()\n",
    "for index,row in distframe.iterrows():\n",
    "    sub_mins[index] = list(distframe.columns[row.argsort()][0:100])\n",
    "    sub_maxs[index] = list(distframe.columns[row.argsort()][-99:])\n",
    "    \n",
    "sub_mins=sub_mins.transpose()\n",
    "sub_maxs=sub_maxs.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_maxs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform statistical analysis on our data, we read in small data set. Both .csv files contain the same users. One file contains a user's comment frequency data, and the other contains that same user's submission frequency data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donaldc = pd.read_csv('donaldCFreq.csv')\n",
    "donalds = pd.read_csv('donaldSFreq.csv')\n",
    "\n",
    "print(\"DonaldCFreq:\")\n",
    "print(donaldc)\n",
    "print(\"donaldSFreq:\")\n",
    "print(donalds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for donaldc\n",
    "donaldc[\"total_comments\"] = donaldc.sum(axis = 1)\n",
    "donaldc[\"total_comments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for donalds\n",
    "donalds[\"total_submissions\"] = donalds.sum(axis = 1)\n",
    "donalds[\"total_submissions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donaldCombined = pd.DataFrame()\n",
    "donaldCombined[\"#Comments\"] = donaldc[\"total_comments\"]\n",
    "donaldCombined[\"#Submissions\"] = donalds[\"total_submissions\"]\n",
    "donaldCombined = donaldCombined.dropna()\n",
    "donaldCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete submissions > 800:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donaldCombined = donaldCombined[donaldCombined[\"#Submissions\"] < 800] \n",
    "donaldCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate Z scores to normalize the data, so we can perform statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "donaldCombined['#Comments_Z'] = scipy.stats.zscore(donaldCombined['#Comments'])\n",
    "donaldCombined['#Submissions_Z'] = scipy.stats.zscore(donaldCombined['#Submissions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot of Z scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot of Z scores\n",
    "plt.scatter(donaldCombined['#Comments_Z'], donaldCombined['#Submissions_Z'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Pearson Correlation Coefficient to find correlation between two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donaldCombined['#Comments_Z'].corr(donaldCombined['#Submissions_Z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient is about -0.25, making it somewhat anti-correlated. While this is not a very strong anti-correlation, it might still be assumed that users who comment a lot do not post submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
